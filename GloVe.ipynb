{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU,SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pd.read_csv(\"positive.csv\", header=None, error_bad_lines=False, sep=';', names = [str(x) for x in (list(range(3)) + ['Text'] + list(range(4,12)))])\n",
    "pos[\"emotion\"] = 1\n",
    "\n",
    "neg = pd.read_csv(\"negative.csv\", header=None, error_bad_lines=False, sep=';', names = [str(x) for x in (list(range(3)) + ['Text'] + list(range(4,12)))])\n",
    "neg[\"emotion\"] = 0\n",
    "\n",
    "df = pd.concat([neg, pos], ignore_index=True)\n",
    "\n",
    "reg = re.compile('[^а-яА-ЯёЁ\\s.():|!?]')\n",
    "df[\"cleaned\"] = df[\"Text\"].apply(lambda x:re.sub(r'\\s+', ' ', (reg.sub(' ', x))).strip())\n",
    "df[\"cleaned\"] = df[\"cleaned\"].apply(lambda x: re.sub(r'([.():|])',r' \\1 ',x))\n",
    "\n",
    "uTags = {\n",
    "        'A':       'ADJ',                                                                                                                                                                                                                                                                  \n",
    "        'ADV':     'ADV',                                                                                                                                                                                                                                                                  \n",
    "        'ADVPRO':  'ADV',                                                                                                                                                                                                                                                                  \n",
    "        'ANUM':    'ADJ',                                                                                                                                                                                                                                                                    \n",
    "        'APRO':    'DET',                                                                                                                                                                                                                                                                    \n",
    "        'COM':     'ADJ',                                                                                                                                                                                                                                                                  \n",
    "        'CONJ':    'SCONJ',                                                                                                                                                                                                                                                                 \n",
    "        'INTJ':    'INTJ',                                                                                                                                                                                                                                                                 \n",
    "        'NONLEX':  'X',                                                                                                                                                                                                                                                                      \n",
    "        'NUM':     'NUM',                                                                                                                                                                                                                                                                   \n",
    "        'PART':    'PART',                                                                                                                                                                                                                                                                   \n",
    "        'PR':      'ADP',                                                                                                                                                                                                                                                                    \n",
    "        'S':       'NOUN',                                                                                                                                                                                                                                                                   \n",
    "        'SPRO':    'PRON',                                                                                                                                                                                                                                                                  \n",
    "        'UNKN':    'X',                                                                                                                                                                                                                                                                      \n",
    "        'V':       'VERB'\n",
    "    }\n",
    "\n",
    "\n",
    "def sentenceInLemmas(sentence, m=Mystem()):\n",
    "    processed = m.analyze(sentence)\n",
    "    tagged = []\n",
    "    for w in processed:        \n",
    "        try:\n",
    "            lemma = w[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "            pos = w[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "            pos = pos.split('=')[0].strip()\n",
    "            tagged.append(lemma+'_'+uTags[pos])\n",
    "        except:\n",
    "            if 'text' in w:\n",
    "                tagged.append(w['text'].strip())\n",
    "            else:\n",
    "                tagged.append(w)\n",
    "    return re.sub('\\s+',' ',' '.join(tagged)).split(' + ')\n",
    "\n",
    "text = ' + '.join(df[\"cleaned\"].values)\n",
    "df[\"lemmed\"] = sentenceInLemmas(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_index = list(df.loc[df['emotion']==0].index[:15000])+list(df.loc[df['emotion']==1].index[:15000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 30000/30000 [00:09<00:00, 3195.85it/s]\n"
     ]
    }
   ],
   "source": [
    "words_set = set()\n",
    "for i in tqdm(df.iloc[part_index].index):\n",
    "    sentence = df.iloc[i]['lemmed']\n",
    "    for word in sentence.split(' '):\n",
    "        words_set.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = {}\n",
    "for n,word in enumerate(words_set):\n",
    "    word_idx[word]=n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 30000/30000 [00:10<00:00, 2811.04it/s]\n"
     ]
    }
   ],
   "source": [
    "P = np.zeros(shape=(vocab_len,vocab_len))\n",
    "window_size = 5\n",
    "for s in tqdm(df.iloc[part_index]['lemmed'].values):\n",
    "    words = s.split(' ')\n",
    "    for n, word in enumerate(words):\n",
    "        i1 = word_idx[word]\n",
    "        occuring_words_l = words[max([0,n-window_size]):n]\n",
    "        occuring_words_r = words[n+1:min([len(words),n+1+window_size])]\n",
    "        occuring_words = occuring_words_l + occuring_words_r\n",
    "        distances = list(range(len(occuring_words_l),0,-1)) + list(range(1,len(occuring_words_r)+1,1))\n",
    "        weights = [1/x for x in distances]\n",
    "        for ow,w in zip(occuring_words,weights):\n",
    "            i2 = word_idx[ow]\n",
    "            if P[i1,i2] < 100:\n",
    "                P[i1,i2] = min([P[i1,i2]+w,100])\n",
    "P_max = P.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c578b545ab4e58960c4881960e11bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a76b009d406485abe833d63e54bb24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28568.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss before epoch 0: 1243006.1113330657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f79123efc474ecfac4d0267fe52aab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28568.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss before epoch 1: 1008530.8014066451\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2257b479517348bb8957a68223e45cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28568.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss before epoch 2: 929491.2084711127\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f49f25bb7cf4ea78fe1d175c418c8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28568.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss before epoch 3: 884182.9707948067\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6cc41dbbf3404090524f91f8ce5c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28568.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss before epoch 4: 854356.6528184398\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948840e48db7419f93d39812439bf5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28568.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss before epoch 5: 833439.6178608071\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b6c1779a8c43388aa7dff5e5bdddf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28568.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss before epoch 6: 818129.6436833286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86937cf7355a4d419b76d4db5084b7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28568.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss before epoch 7: 806534.5097516429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_dimensionality = 300\n",
    "v_vectors = np.random.uniform(-1,1,(vocab_len, word_dimensionality))\n",
    "u_vectors = np.random.uniform(-1,1,(vocab_len, word_dimensionality))\n",
    "u_learning_rate = 2e-3\n",
    "v_learning_rate = 2e-3\n",
    "\n",
    "n_epochs = 8\n",
    "\n",
    "for step in tqdm_notebook(range(n_epochs)):\n",
    "    # Здесь будут градиенты\n",
    "    u_changes = []\n",
    "    v_changes = []\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for i in tqdm_notebook(range(vocab_len)):\n",
    "        # градиент для конкретного вектора\n",
    "        u_change = None\n",
    "        v_change = None\n",
    "        \n",
    "        for j in range(vocab_len):\n",
    "            P_local = P[i,j]\n",
    "            if P_local == 0:\n",
    "                continue\n",
    "            \n",
    "            # Выражение в скобках, чтобы не пересчитывать несколько раз\n",
    "            diff = np.matmul(u_vectors[i].T, v_vectors[j]) - math.log(P_local)\n",
    "            \n",
    "            loss += (P_local/P_max)**(3/4)*(diff)**2\n",
    "            \n",
    "            v_addition = (P_local/P_max)**(3/4)*diff*u_vectors[j]\n",
    "            u_addition = (P_local/P_max)**(3/4)*diff*v_vectors[j]\n",
    "            \n",
    "            if u_change is not None:\n",
    "                u_change += u_addition\n",
    "                v_change += v_addition\n",
    "            else:\n",
    "                u_change = u_addition\n",
    "                v_change = v_addition\n",
    "        u_changes.append(u_change)\n",
    "        v_changes.append(v_change)\n",
    "        \n",
    "    print('loss before epoch {}: {}'.format(step, loss))\n",
    "        \n",
    "    u_vectors -= u_learning_rate * np.array(u_changes)\n",
    "    v_vectors -= v_learning_rate * np.array(v_changes)\n",
    "    \n",
    "    u_learning_rate *= 0.85\n",
    "    v_learning_rate *= 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 28568/28568 [09:14<00:00, 51.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished at loss: 797510.1406268472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_loss = 0\n",
    "for i in tqdm(range(vocab_len)):\n",
    "    for j in range(vocab_len):\n",
    "        if P[i,j] == 0:\n",
    "            continue\n",
    "        final_loss += (P[i,j]/P_max)**(3/4)*(np.matmul(u_vectors[i].T, v_vectors[j]) - math.log(P[i,j]))**2\n",
    "print('finished at loss: {}'.format(final_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resulting vectors for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = u_vectors + v_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(frame, word2vec):\n",
    "    frame.reset_index(inplace=True)\n",
    "    matrixes = []\n",
    "    tone = []\n",
    "    for i in tqdm_notebook(frame.index):\n",
    "        try:\n",
    "            words = frame.iloc[i]['lemmed'].split(' ')\n",
    "            word_vectors = []\n",
    "            for w in words:\n",
    "                try:\n",
    "                    word_vectors.append(word2vec[word_idx[w]])\n",
    "                except KeyError:\n",
    "                    print(w)\n",
    "            if len(word_vectors) > 2 and len(word_vectors) <= 32:\n",
    "                matrixes.append(np.array(word_vectors))\n",
    "                tone.append(frame.iloc[i]['emotion'])\n",
    "        except Exception as e:\n",
    "            print(frame.iloc[i]['lemmed'])\n",
    "            raise e\n",
    "\n",
    "    return matrixes, tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabsize = 300\n",
    "timesteps = 32\n",
    "batch_size = 64\n",
    "n_epochs = 200\n",
    "n_parts = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573806e01fdf4159a2ad7a22de3dad23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "matrices, tones = to_matrix(df.iloc[part_index], word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6cd85bace544b719031888ad6bf2acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29753.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm_notebook(range(len(matrices))):\n",
    "    matrices[i] = np.vstack(\n",
    "        (np.random.normal(scale=0.005, size=(timesteps - matrices[i].shape[0], vocabsize)),matrices[i]))\n",
    "matrices = np.array(matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(matrices, tones, test_size=0.2, random_state=42,stratify=tones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train, ndmin=2).T\n",
    "y_test = np.array(y_test, ndmin=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(timesteps, vocabsize):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(1, return_sequences=False, input_shape=(timesteps, vocabsize), dropout=0.05, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = lstm(timesteps, vocabsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23802 samples, validate on 5951 samples\n",
      "Epoch 1/50\n",
      " - 19s - loss: 0.2666 - acc: 0.8600 - val_loss: 0.2156 - val_acc: 0.8704\n",
      "Epoch 2/50\n",
      " - 17s - loss: 0.1997 - acc: 0.8816 - val_loss: 0.1908 - val_acc: 0.8852\n",
      "Epoch 3/50\n",
      " - 17s - loss: 0.1808 - acc: 0.9006 - val_loss: 0.1831 - val_acc: 0.9020\n",
      "Epoch 4/50\n",
      " - 18s - loss: 0.1719 - acc: 0.9190 - val_loss: 0.1769 - val_acc: 0.9027\n",
      "Epoch 5/50\n",
      " - 18s - loss: 0.1663 - acc: 0.9324 - val_loss: 0.1684 - val_acc: 0.9361\n",
      "Epoch 6/50\n",
      " - 18s - loss: 0.1604 - acc: 0.9444 - val_loss: 0.1651 - val_acc: 0.9366\n",
      "Epoch 7/50\n",
      " - 17s - loss: 0.1564 - acc: 0.9484 - val_loss: 0.1609 - val_acc: 0.9456\n",
      "Epoch 8/50\n",
      " - 17s - loss: 0.1533 - acc: 0.9517 - val_loss: 0.1589 - val_acc: 0.9504\n",
      "Epoch 9/50\n",
      " - 18s - loss: 0.1503 - acc: 0.9576 - val_loss: 0.1531 - val_acc: 0.9524\n",
      "Epoch 10/50\n",
      " - 17s - loss: 0.1466 - acc: 0.9605 - val_loss: 0.1501 - val_acc: 0.9580\n",
      "Epoch 11/50\n",
      " - 16s - loss: 0.1433 - acc: 0.9637 - val_loss: 0.1474 - val_acc: 0.9578\n",
      "Epoch 12/50\n",
      " - 18s - loss: 0.1406 - acc: 0.9656 - val_loss: 0.1437 - val_acc: 0.9639\n",
      "Epoch 13/50\n",
      " - 18s - loss: 0.1375 - acc: 0.9682 - val_loss: 0.1419 - val_acc: 0.9657\n",
      "Epoch 14/50\n",
      " - 18s - loss: 0.1351 - acc: 0.9684 - val_loss: 0.1403 - val_acc: 0.9652\n",
      "Epoch 15/50\n",
      " - 18s - loss: 0.1328 - acc: 0.9700 - val_loss: 0.1395 - val_acc: 0.9667\n",
      "Epoch 16/50\n",
      " - 18s - loss: 0.1315 - acc: 0.9714 - val_loss: 0.1357 - val_acc: 0.9667\n",
      "Epoch 17/50\n",
      " - 17s - loss: 0.1304 - acc: 0.9730 - val_loss: 0.1351 - val_acc: 0.9691\n",
      "Epoch 18/50\n",
      " - 18s - loss: 0.1275 - acc: 0.9726 - val_loss: 0.1331 - val_acc: 0.9679\n",
      "Epoch 19/50\n",
      " - 18s - loss: 0.1261 - acc: 0.9735 - val_loss: 0.1351 - val_acc: 0.9679\n",
      "Epoch 20/50\n",
      " - 18s - loss: 0.1243 - acc: 0.9739 - val_loss: 0.1321 - val_acc: 0.9729\n",
      "Epoch 21/50\n",
      " - 18s - loss: 0.1210 - acc: 0.9753 - val_loss: 0.1305 - val_acc: 0.9701\n",
      "Epoch 22/50\n",
      " - 17s - loss: 0.1207 - acc: 0.9753 - val_loss: 0.1276 - val_acc: 0.9738\n",
      "Epoch 23/50\n",
      " - 17s - loss: 0.1188 - acc: 0.9767 - val_loss: 0.1276 - val_acc: 0.9746\n",
      "Epoch 24/50\n",
      " - 18s - loss: 0.1169 - acc: 0.9763 - val_loss: 0.1269 - val_acc: 0.9755\n",
      "Epoch 25/50\n",
      " - 19s - loss: 0.1168 - acc: 0.9765 - val_loss: 0.1243 - val_acc: 0.9753\n",
      "Epoch 26/50\n",
      " - 20s - loss: 0.1145 - acc: 0.9773 - val_loss: 0.1246 - val_acc: 0.9734\n",
      "Epoch 27/50\n",
      " - 20s - loss: 0.1123 - acc: 0.9772 - val_loss: 0.1237 - val_acc: 0.9760\n",
      "Epoch 28/50\n",
      " - 19s - loss: 0.1122 - acc: 0.9773 - val_loss: 0.1199 - val_acc: 0.9750\n",
      "Epoch 29/50\n",
      " - 19s - loss: 0.1091 - acc: 0.9780 - val_loss: 0.1191 - val_acc: 0.9758\n",
      "Epoch 30/50\n",
      " - 18s - loss: 0.1092 - acc: 0.9778 - val_loss: 0.1182 - val_acc: 0.9761\n",
      "Epoch 31/50\n",
      " - 18s - loss: 0.1069 - acc: 0.9786 - val_loss: 0.1184 - val_acc: 0.9755\n",
      "Epoch 32/50\n",
      " - 19s - loss: 0.1060 - acc: 0.9787 - val_loss: 0.1165 - val_acc: 0.9760\n",
      "Epoch 33/50\n",
      " - 19s - loss: 0.1030 - acc: 0.9794 - val_loss: 0.1167 - val_acc: 0.9761\n",
      "Epoch 34/50\n",
      " - 19s - loss: 0.1038 - acc: 0.9792 - val_loss: 0.1127 - val_acc: 0.9766\n",
      "Epoch 35/50\n",
      " - 19s - loss: 0.1016 - acc: 0.9803 - val_loss: 0.1171 - val_acc: 0.9760\n",
      "Epoch 36/50\n",
      " - 19s - loss: 0.1009 - acc: 0.9797 - val_loss: 0.1142 - val_acc: 0.9770\n",
      "Epoch 37/50\n",
      " - 20s - loss: 0.0985 - acc: 0.9803 - val_loss: 0.1098 - val_acc: 0.9773\n",
      "Epoch 38/50\n",
      " - 19s - loss: 0.0969 - acc: 0.9803 - val_loss: 0.1085 - val_acc: 0.9773\n",
      "Epoch 39/50\n",
      " - 19s - loss: 0.0960 - acc: 0.9799 - val_loss: 0.1073 - val_acc: 0.9780\n",
      "Epoch 40/50\n",
      " - 18s - loss: 0.0963 - acc: 0.9805 - val_loss: 0.1088 - val_acc: 0.9771\n",
      "Epoch 41/50\n",
      " - 17s - loss: 0.0941 - acc: 0.9804 - val_loss: 0.1051 - val_acc: 0.9775\n",
      "Epoch 42/50\n",
      " - 17s - loss: 0.0942 - acc: 0.9804 - val_loss: 0.1065 - val_acc: 0.9761\n",
      "Epoch 43/50\n",
      " - 16s - loss: 0.0928 - acc: 0.9805 - val_loss: 0.1041 - val_acc: 0.9770\n",
      "Epoch 44/50\n",
      " - 16s - loss: 0.0934 - acc: 0.9807 - val_loss: 0.1034 - val_acc: 0.9768\n",
      "Epoch 45/50\n",
      " - 16s - loss: 0.0894 - acc: 0.9812 - val_loss: 0.1003 - val_acc: 0.9782\n",
      "Epoch 46/50\n",
      " - 18s - loss: 0.0885 - acc: 0.9811 - val_loss: 0.0978 - val_acc: 0.9775\n",
      "Epoch 47/50\n",
      " - 17s - loss: 0.0855 - acc: 0.9811 - val_loss: 0.0972 - val_acc: 0.9780\n",
      "Epoch 48/50\n",
      " - 16s - loss: 0.0840 - acc: 0.9812 - val_loss: 0.0962 - val_acc: 0.9785\n",
      "Epoch 49/50\n",
      " - 16s - loss: 0.0841 - acc: 0.9815 - val_loss: 0.0995 - val_acc: 0.9782\n",
      "Epoch 50/50\n",
      " - 17s - loss: 0.0823 - acc: 0.9823 - val_loss: 0.0938 - val_acc: 0.9785\n",
      "Wall time: 14min 51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x211437412b0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model3.fit(X_train, y_train, batch_size=batch_size, epochs=50, verbose=2,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
